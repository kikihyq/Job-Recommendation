{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56Z6PDQ-HbgL"
   },
   "source": [
    "# Tutorial 4 (Week 5) - Supervised and Unsupervised Learning\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "After completing this tutorial, you should be able to:\n",
    "\n",
    "+ Prepare data for learning\n",
    "  + Perform train-test split\n",
    "  + Perform punctuation and stopword removal\n",
    "  + Perform feature generation and vocabulary learning\n",
    "  + Write custom transformer\n",
    "+ Perform supervised learning\n",
    "  + Perform classification using Naive Bayes\n",
    "  + Perform regression\n",
    "+ Perform unsupervised learning\n",
    "  + Perform k-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJTDMgYKHbgW"
   },
   "source": [
    "# Supervised Learning - Spam Classification Example\n",
    "\n",
    "## Dataset\n",
    "\n",
    "Let's try to handle a new type of data: text data. Given a collection of text messages, we want to be able to classify whether a message is a spam or not. This is a binary classification problem. \n",
    "\n",
    "We use the [SMS Spam Collection](https://archive.ics.uci.edu/dataset/228/sms+spam+collection) dataset from UC Irvine. The text data file and the accompanying `readme` file have been downloaded for you in the tutorial folder. Check the `readme` file to understand the data format.\n",
    "\n",
    "Let's first load the dataset into a DataFrame named `sms`. Our usual `read_csv` function also supports loading text data. Check the function documentation and set the correct parameters to load this dataset properly.\n",
    "\n",
    "_(Hint: What is the separator? Does it have headers?)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "OeqQrDRQHbgX",
    "outputId": "7267ca59-da37-4447-8b47-da06c2840234"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0                                                  1\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "sms = pd.read_csv('T4-SMSSpamCollection',sep='\\t',header=None)\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VH9-NmiQ3NJH"
   },
   "source": [
    "The first column contains the labels for message in the second column: \"spam\" if the message is a spam, or \"ham\" if it is not a spam.\n",
    "\n",
    "Let's clean the dataset by renaming the columns with meaningful headers `label` and `message`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "sftJmGyyHbgc",
    "outputId": "8ab3d7c4-8ecb-4140-dabe-f2ba61535be0",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "sms = sms.rename(columns={0:'label',1:'message'})\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "09-M1QpsZGtK"
   },
   "source": [
    "## Train-Test Split\n",
    "\n",
    "To avoid overfitting, it is common practice when performing a supervised machine learning experiment to hold out part of the available data as a _test set_, to be used for evaluating the model performance after training. \n",
    "\n",
    "__It is important that the splitting of data into training and test subsets are done _before_ any preprocessing.__ That is, data preprocessing steps (which we learned in the previous tutorial) should be applied to the training set and the test set separately. This prevents _data leakage_, where information from test data is used to make choices when building the model, resulting in overly optimistic performance estimates.\n",
    "\n",
    "In scikit-learn, a random split into training and test sets can be quickly computed with the [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split) helper function.\n",
    "\n",
    "Whenever randomization is part of a Scikit-learn algorithm, a `random_state` parameter may be provided to control the random number generator used. In order to obtain reproducible (i.e., constant) results across multiple program executions, we need to remove all uses of `random_state=None`, which is the default. The recommended way in sklearn is to declare a `rng` variable at the top of the program, and pass it down to any object that accepts a `random_state` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "bFalcV4fHbgk"
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sms_train, sms_test = train_test_split( sms, test_size=0.20, random_state=rng ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ZiZ5oQYHbgl",
    "outputId": "919e3996-5401-41fe-c00e-79160107ff32",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of samples in training set: 4457\n",
      "No of samples in test set: 1115\n"
     ]
    }
   ],
   "source": [
    "print( \"No of samples in training set: %s\" % len(sms_train) )\n",
    "print( \"No of samples in test set: %s\" % len(sms_test) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, we use a split ratio of 80:20 (specified in the `test_size` parameter), which means 80% of the data is used for training and 20% is held out for testing. This ratio is commonly used and is usually a good starting point.\n",
    "\n",
    "In general, determining the appropriate ratio for a machine learning problem involves considerations such as having sufficient data size, ensuring that the sets have satisfactory variance and adequate representation of the actual distribution, computational costs, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xAVY5qQfHbgu"
   },
   "source": [
    "## Training Data Preprocessing\n",
    "\n",
    "As the aim of this tutorial is not preprocessing, we will do quick operations and majorly focus on handling text data and learning to train a model.\n",
    "\n",
    "Let us first get quick descriptive statistics of the training set. The statistics summary of this text data will look rather different from that of numerical data that we have mostly seen in previous tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "dJJJQFiPHbgv",
    "outputId": "71a9818b-952e-4bd1-974e-a82359701222",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4457</td>\n",
       "      <td>4457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>4206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>3870</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                 message\n",
       "count   4457                    4457\n",
       "unique     2                    4206\n",
       "top      ham  Sorry, I'll call later\n",
       "freq    3870                      26"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "sms_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also check the descriptive statistics for individual `ham` and `spam` categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "KFVdLHacHbgw",
    "outputId": "2125a2a8-a2f1-4c6d-8bf2-d5d0a84a7a1f",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">message</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>3870</td>\n",
       "      <td>3668</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>587</td>\n",
       "      <td>538</td>\n",
       "      <td>I don't know u and u don't know me. Send CHAT ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      message                                                               \n",
       "        count unique                                                top freq\n",
       "label                                                                       \n",
       "ham      3870   3668                             Sorry, I'll call later   26\n",
       "spam      587    538  I don't know u and u don't know me. Send CHAT ...    3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "sms_train.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-2N1NHOHbg0"
   },
   "source": [
    "### Punctuation and Stopword Removal\n",
    "\n",
    "Python's `string` library has a pre-defined constant string `punctuation` that contains all punctuation characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more than one ways to remove punctuations from a string. In this tutorial, we will use an approach based on [regular expression](https://docs.python.org/3/library/re.html#module-re) (regex), which essentially matches the defined regex pattern for punctuations and replaces it with empty string.\n",
    "\n",
    "In this case, the regex pattern is simply the occurence of any character from `string.punctuation`, plus other characters we want to discard for our purpose. For our spam message classification task, we will also discard digits, tab and newline characters.\n",
    "\n",
    "The following code compiles this pattern into a regex object, which we can then use on our text messages. Note that special characters such as those in `string.punctuation` need to be escaped in the regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[!\"\\\\#\\\\$%\\\\&\\'\\\\(\\\\)\\\\*\\\\+,\\\\-\\\\./:;<=>\\\\?@\\\\[\\\\\\\\\\\\]\\\\^_`\\\\{\\\\|\\\\}\\\\~0-9\\\\r\\\\t\\\\n]'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "regex = re.compile( '[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n'+']' )\n",
    "regex.pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-2N1NHOHbg0"
   },
   "source": [
    "__Stopword__ refers to commonly used words, such as \"a\", \"the\", \"is\", etc. These words are not providing very useful information and hence are generally removed during preprocessing.\n",
    "\n",
    "The [Natural Language Toolkit (NLTK)](https://www.nltk.org/) library has a list of stopwords that we can use to filter out stopwords from the text messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3q5ny7SCHbg0",
    "outputId": "0e4ea768-0801-42ea-b7e5-5538a9fde36d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HYQ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "#stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us write a function `textProcess` that takes in a text input, removes punctuation and stopwords, and returns the count of remaining words. Complete the code segment below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3q5ny7SCHbg0",
    "outputId": "0e4ea768-0801-42ea-b7e5-5538a9fde36d"
   },
   "outputs": [],
   "source": [
    "def textProcess( text, verbose=False ): \n",
    "    \n",
    "    try:\n",
    "        if verbose==True:\n",
    "            print( text )\n",
    "            \n",
    "        # TODO: Convert the text to lowercase first\n",
    "        # text = ?\n",
    "        text = text.lower()\n",
    "        \n",
    "        if verbose==True:\n",
    "            print( text )\n",
    "\n",
    "        # TODO: Remove punctuation using the regex object\n",
    "        # text = ?\n",
    "        text = regex.sub('',text)\n",
    "        \n",
    "        if verbose==True:\n",
    "            print( text )\n",
    "        \n",
    "        # TODO: Remove stopwords: split text into individual words, and save non-stopwords into an array\n",
    "        # words = ?\n",
    "        words = [w for w in text.split(' ') if w not in stop]\n",
    "        \n",
    "        if verbose==True:\n",
    "            print( words )\n",
    "        \n",
    "        # TODO: Return the count of remaining words\n",
    "        return len(words)\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our function on a sample message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YY7pA9CVHbg2",
    "outputId": "ad6ec161-6479-4533-e0b8-f023caffd7cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New car and house for my parents.:)i have only new job in hand:)\n",
      "new car and house for my parents.:)i have only new job in hand:)\n",
      "new car and house for my parentsi have only new job in hand\n",
      "['new', 'car', 'house', 'parentsi', 'new', 'job', 'hand']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textProcess( sms_train['message'].iloc[199], verbose=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Generation\n",
    "\n",
    "Let us now generate a few new features and store them as new columns:\n",
    "- `length`: character count of the original text message\n",
    "- `word_count`: word count of the original text message\n",
    "- `processed_word_count`: word count of the processed text message\n",
    "\n",
    "We can use [`Series.apply`](https://pandas.pydata.org/docs/reference/api/pandas.Series.apply.html) to invoke the desired functions on each data value in the Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Gt_DN5AdHbgy",
    "outputId": "60349fd4-5725-43ff-c4cc-fa149bf6ec1b",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1114</th>\n",
       "      <td>ham</td>\n",
       "      <td>No I'm good for the movie, is it ok if I leave...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3589</th>\n",
       "      <td>ham</td>\n",
       "      <td>If you were/are free i can give. Otherwise nal...</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3095</th>\n",
       "      <td>ham</td>\n",
       "      <td>Have you emigrated or something? Ok maybe 5.30...</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>ham</td>\n",
       "      <td>I just got home babe, are you still awake ?</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3320</th>\n",
       "      <td>ham</td>\n",
       "      <td>Kay... Since we are out already</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                            message  length\n",
       "1114   ham  No I'm good for the movie, is it ok if I leave...      61\n",
       "3589   ham  If you were/are free i can give. Otherwise nal...      72\n",
       "3095   ham  Have you emigrated or something? Ok maybe 5.30...      67\n",
       "1012   ham        I just got home babe, are you still awake ?      43\n",
       "3320   ham                   Kay... Since we are out already       32"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Generate length feature\n",
    "sms_train['length'] = sms_train['message'].apply(len)\n",
    "sms_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "XrqqseLwHbg3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1114</th>\n",
       "      <td>ham</td>\n",
       "      <td>No I'm good for the movie, is it ok if I leave...</td>\n",
       "      <td>61</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3589</th>\n",
       "      <td>ham</td>\n",
       "      <td>If you were/are free i can give. Otherwise nal...</td>\n",
       "      <td>72</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3095</th>\n",
       "      <td>ham</td>\n",
       "      <td>Have you emigrated or something? Ok maybe 5.30...</td>\n",
       "      <td>67</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>ham</td>\n",
       "      <td>I just got home babe, are you still awake ?</td>\n",
       "      <td>43</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3320</th>\n",
       "      <td>ham</td>\n",
       "      <td>Kay... Since we are out already</td>\n",
       "      <td>32</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                            message  length  \\\n",
       "1114   ham  No I'm good for the movie, is it ok if I leave...      61   \n",
       "3589   ham  If you were/are free i can give. Otherwise nal...      72   \n",
       "3095   ham  Have you emigrated or something? Ok maybe 5.30...      67   \n",
       "1012   ham        I just got home babe, are you still awake ?      43   \n",
       "3320   ham                   Kay... Since we are out already       32   \n",
       "\n",
       "      word_count  \n",
       "1114          15  \n",
       "3589          13  \n",
       "3095          12  \n",
       "1012          10  \n",
       "3320           7  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Generate word_count feature\n",
    "sms_train['word_count'] = sms_train['message'].apply(lambda x:len( [w for w in x.split(' ')]  ) )\n",
    "sms_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "XrqqseLwHbg3"
   },
   "outputs": [],
   "source": [
    "# TODO: Generate preprocessed_word_count feature\n",
    "sms_train['processed_word_count'] =sms_train['message'].apply(lambda x: textProcess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "QZLzV-fQHbg4",
    "outputId": "e4f955e9-e639-40e6-ac60-72dcc7f9f8d3",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>processed_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1114</th>\n",
       "      <td>ham</td>\n",
       "      <td>No I'm good for the movie, is it ok if I leave...</td>\n",
       "      <td>61</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3589</th>\n",
       "      <td>ham</td>\n",
       "      <td>If you were/are free i can give. Otherwise nal...</td>\n",
       "      <td>72</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3095</th>\n",
       "      <td>ham</td>\n",
       "      <td>Have you emigrated or something? Ok maybe 5.30...</td>\n",
       "      <td>67</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>ham</td>\n",
       "      <td>I just got home babe, are you still awake ?</td>\n",
       "      <td>43</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3320</th>\n",
       "      <td>ham</td>\n",
       "      <td>Kay... Since we are out already</td>\n",
       "      <td>32</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                            message  length  \\\n",
       "1114   ham  No I'm good for the movie, is it ok if I leave...      61   \n",
       "3589   ham  If you were/are free i can give. Otherwise nal...      72   \n",
       "3095   ham  Have you emigrated or something? Ok maybe 5.30...      67   \n",
       "1012   ham        I just got home babe, are you still awake ?      43   \n",
       "3320   ham                   Kay... Since we are out already       32   \n",
       "\n",
       "      word_count  processed_word_count  \n",
       "1114          15                     6  \n",
       "3589          13                     9  \n",
       "3095          12                     7  \n",
       "1012          10                     6  \n",
       "3320           7                     4  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put these three features that we have generated into a NumPy array, in preparation to build our training dataset (the `x` in our machine learning recipe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 61,  15,   6],\n",
       "       [ 72,  13,   9],\n",
       "       [ 67,  12,   7],\n",
       "       ...,\n",
       "       [122,  19,  15],\n",
       "       [ 56,  15,   9],\n",
       "       [ 53,  10,   7]], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = sms_train[['length', 'word_count', 'processed_word_count']].to_numpy()\n",
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also encode the `spam`/`ham` labels as numeric values `1`/`0` and save them in an array (the `y` in our machine learning recipe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = [1 if l==\"spam\" else 0 for l in sms_train['label']]\n",
    "#y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rC_ghUJCHbhK"
   },
   "source": [
    "### Generating Vocabulary Feature\n",
    "\n",
    "The [sklearn.feature_extraction](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction) module includes submodules dedicated to feature extraction from images and text. \n",
    "\n",
    "A useful and simple utility in the `sklearn.feature_extraction.text` submodule is [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer). It implements both tokenization and occurrence counting in a single class. It can also be configured to perform preprocessing such as stopwords removal and character normalization. In this tutorial, we will just use it to generate a word count vector.\n",
    "\n",
    "The `fit_transform` method of `CountVectorizer` learns the vocabulary dictionary and return the document-term matrix. Each term found during the fit is assigned a unique integer index corresponding to a column in the resulting matrix.\n",
    "\n",
    "![CountVectorizer](https://www.educative.io/api/edpresso/shot/5197621598617600/image/6596233398321152)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hp-l8CMqHbhK",
    "outputId": "3f988425-523a-41dd-cd0e-208242b40b89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of samples in the training set: 4457\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4457, 7793)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform( sms_train.message )\n",
    "\n",
    "print( \"No of samples in the training set: %s\" % len(sms_train) )\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dEnj-oF_57aZ"
   },
   "source": [
    "We can view the features in the `CountVectorizer` transformation (i.e., the vocabulary learned) using `get_feature_names_out()`. The length of this list should correspond to the number of columns in `X_train_counts` (refer to the illustration above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IcyhcGJUd7Ns",
    "outputId": "ec41bf81-9556-41bc-94d2-b08e63e467bc"
   },
   "outputs": [],
   "source": [
    "# TODO: vocab = ?\n",
    "vocab = count_vect.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IcyhcGJUd7Ns",
    "outputId": "ec41bf81-9556-41bc-94d2-b08e63e467bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7793\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['anyones', 'anyplaces', 'anythin', 'anything', 'anythingtomorrow',\n",
       "       'anytime', 'anyway', 'anyways', 'anywhere', 'aom', 'apart',\n",
       "       'apartment', 'apes', 'aphex', 'apnt', 'apo', 'apologise',\n",
       "       'apologize', 'app', 'apparently', 'appeal', 'appear', 'appendix',\n",
       "       'applebees', 'apples', 'application', 'apply', 'applyed',\n",
       "       'applying', 'appointment', 'appointments', 'appreciate',\n",
       "       'appreciated', 'approaches', 'approaching', 'appropriate',\n",
       "       'approve', 'approved', 'approx', 'apps', 'appt', 'appy', 'april',\n",
       "       'aproach', 'apt', 'aptitude', 'aquarius', 'ar', 'arab', 'arabian',\n",
       "       'arcade', 'ard', 'are', 'area', 'aren', 'arent', 'arestaurant',\n",
       "       'aretaking', 'areyouunique', 'argentina', 'argh', 'argue',\n",
       "       'arguing', 'argument', 'arguments', 'aries', 'arise', 'arises',\n",
       "       'arithmetic', 'arm', 'armand', 'armenia', 'arms', 'arng', 'arngd',\n",
       "       'arnt', 'around', 'aroundn', 'arr', 'arrange', 'arrested',\n",
       "       'arrive', 'arrived', 'arrow', 'arsenal', 'art', 'artists', 'arts',\n",
       "       'arun', 'as', 'asa', 'asap', 'asda', 'ashes', 'ashley', 'ashwini',\n",
       "       'asia', 'asian', 'asjesus', 'ask'], dtype=object)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print( len(vocab) )\n",
    "vocab[1000:1100] # view a subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to check the feature vector generated for one of the messages, say at index 900."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "hDWAuWMGeRmV",
    "outputId": "ab43c5ca-c4dd-46d1-fa84-67a77f0177b5",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I jus hope its true that  missin me cos i'm really missin him! You haven't done anything to feel guilty about, yet.\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_train['message'].iloc[900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UXCmOkuHe7mN",
    "outputId": "6154fc20-daef-40cd-9038-fae57dbf1165"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cts = X_train_counts.toarray()\n",
    "cts[900]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The array has nonzero values only at indices corresponding to the words in the vocabulary, where the values indicate the number of occurences of that word in the message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UXCmOkuHe7mN",
    "outputId": "6154fc20-daef-40cd-9038-fae57dbf1165"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 763, 1003, 2018, 2409, 2804, 3296, 3376, 3453, 3508, 3793, 3903,\n",
       "        4433, 4535, 5642, 6862, 6986, 7098, 7743, 7756], dtype=int64),)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonzero_idx = np.nonzero( cts[900] )\n",
    "nonzero_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the vocabulary word list (`vocab`), vocabulary count array for the message (`cts[900]`), and indices of nonzero values for the message (`nonzero_idx`), can you find out the following?\n",
    "- Total word count of the message\n",
    "- The list of words in the message\n",
    "- The count of each word in the message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UXCmOkuHe7mN",
    "outputId": "6154fc20-daef-40cd-9038-fae57dbf1165"
   },
   "outputs": [],
   "source": [
    "# TODO: Total word count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UXCmOkuHe7mN",
    "outputId": "6154fc20-daef-40cd-9038-fae57dbf1165"
   },
   "outputs": [],
   "source": [
    "# TODO: List of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UXCmOkuHe7mN",
    "outputId": "6154fc20-daef-40cd-9038-fae57dbf1165",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Count of each word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This vocabulary that `count_vect` has learned is now stored as its attribute. Any other words that are not part of this list (i.e., not seen in the training set) will be completely ignored in future calls to its `transform` method (e.g., when run on the test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pIL_0dRz6tyK",
    "outputId": "57036334-2ed8-4146-850f-348033582f84"
   },
   "outputs": [],
   "source": [
    "count_vect.transform([\"alamak\"]).toarray().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1IBmEF0lHbhN"
   },
   "source": [
    "Let us now create the training dataset by putting together this vocabulary count vector with the character/word count features that we had earlier. Do note that essentially the vocabulary count vector is also providing us information regarding the count of the words. We do not really expect to see huge improvements with this approach. But we are continuing in this tutorial so as to learn about mixing and using such differently created features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SY4P0q9LHbhO"
   },
   "outputs": [],
   "source": [
    "trainData = np.hstack( (cts, x_train) )\n",
    "trainData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CLHXy4CxHbg_"
   },
   "source": [
    "## Test Data Preprocessing\n",
    "\n",
    "All data transformations that are applied when training a model must also be used on subsequent datasets, whether it is the test set or data in a production system. Otherwise, the feature space will change, and the model will not be able to perform effectively.\n",
    "\n",
    "Let us first try to apply one by one, the same preprocessing steps that we have done on the training dataset, to the test set. We start by generating the array of character and word count features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7IL6nzoVHbhA"
   },
   "outputs": [],
   "source": [
    "# TODO: Generate length, word_count, and processed_word_count features for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7IL6nzoVHbhA"
   },
   "outputs": [],
   "source": [
    "# TODO: x_test = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encode the `spam`/`ham` labels as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7IL6nzoVHbhA",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: y_test = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to be careful when applying transformations.\n",
    "\n",
    "Although both train and test data subsets should receive the same preprocessing transformations, __it is important that these transformations are only learnt from the training data__. For example, if we have a normalization step that divides by the average value, the average should be the average of the training subset, not the average of all the data. If the test subset is included in the average calculation, information from the test subset is influencing the model, that is, the _data leakage_ problem described earlier.\n",
    "\n",
    "In other words, the general rule is to __never call `fit` on the test data__. \n",
    "\n",
    "Thus, to generate the vocabulary count vector for the test data, we use the same `CountVectorizer` transformer initialized earlier, but apply `transform` only instead of `fit_transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "noWCljOiHbhM"
   },
   "outputs": [],
   "source": [
    "# TODO: X_test_counts = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ov2j4GYkHbhN",
    "outputId": "e093ce00-f1f2-48d2-c881-9e12cb263383",
    "scrolled": true
   },
   "source": [
    "Finally, we create the test data from the character/word counts and the vocabulary counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SY4P0q9LHbhO"
   },
   "outputs": [],
   "source": [
    "# TODO: testData = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTdSUm_9HbhO"
   },
   "source": [
    "We are now ready to perform some classification.\n",
    "\n",
    "## Classification using Naive Bayes\n",
    "\n",
    "Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes’ theorem with the “naive” assumption of conditional independence between every pair of features given the value of the class variable. \n",
    "\n",
    "If `y` is the prediction, and the `x`-es are the features, then Bayes' theorem gives the conditional probability of `y` given `x`. Using the conditional assumption among features, the equations are simplified to provide us an estimate of `y`. The different Naive Bayes algorithms typically differ in the assumption of the distribution of feature given the `y`.\n",
    "\n",
    "In this tutorial, we do not aim to understand a specific classifier, or its working. The aim is to understand how we can experiment with the features and perform predictions using `sklearn`. Once the implementation is understood, the classifiers in `sklearn` can be changed according to the problem at hand. \n",
    "\n",
    "In this tutorial, we will try two different Naive Bayes algorithms available in `sklearn`: _Multinomial Naive Bayes_ and _Complement Naive Bayes_. The [User Guide](https://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes) is a useful resource to find simple explanations regarding what can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xqN7-dFtHbhP",
    "outputId": "80010ae1-6be4-4cb4-c692-dd37abc2956d"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes\n",
    "\n",
    "Let's try this classifier on the training data set with fewer features, `x_train`. We start by instantiating the classifier and fitting it to the training data and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xqN7-dFtHbhP",
    "outputId": "80010ae1-6be4-4cb4-c692-dd37abc2956d"
   },
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit( x_train, y_train )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a function to let us examine the confusion matrix and performance scores of the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-gwMzZxXHbhR"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, confusion_matrix, accuracy_score\n",
    "\n",
    "def evaluate( y_pred, y_test ):\n",
    "    \n",
    "    print( \"-----------\" )\n",
    "\n",
    "    c = confusion_matrix( y_test, y_pred ) #[[TN, FP],[FN,TP]]\n",
    "    tn, fp, fn, tp = c.ravel() # returns a flattened array\n",
    "\n",
    "    print( c )\n",
    "    print( \"-----------\" )\n",
    "    \n",
    "    print( \"Accuracy:\", str( accuracy_score( y_test, y_pred )))\n",
    "    \n",
    "    sens, spec = tp/(tp+fn), tn/(tn+fp) \n",
    "    print( \"Specificity: {0}, Sensitivity: {1}\".format(spec, sens) )\n",
    "    print( \"Precision:\", str( precision_score( y_test, y_pred )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we perform the prediction on the corresponding `x_test` and evaluate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_zFPbBcPHbhR",
    "outputId": "739154bb-feb7-4b9a-9025-89a5b1ea368e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict( x_test )\n",
    "\n",
    "evaluate( y_pred, y_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complement Naive Bayes\n",
    "\n",
    "Complement Naive Bayes is an adaptation of the standard Multinomial Naive Bayes algorithm that is particularly suited for imbalanced data sets. Imbalanced datasets are datasets where the number of examples of some class is higher than the number of examples belonging to other classes.\n",
    "\n",
    "Let's try this classifier on the training data set with more features, `trainData`. Similarly, start by instantiating the classifier and fitting it to the training data and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p20hqec8HbhS",
    "outputId": "42130887-9253-4b5e-8111-3bfb805a4c37"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now perform the prediction on the corresponding test data, and evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r7sLyewwHbhS",
    "outputId": "1d1161d1-37d0-4bad-81db-e675173c5c14",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4gOXBn4HbhS"
   },
   "source": [
    "## Chaining Estimators\n",
    "\n",
    "In the above, we have applied preprocessing steps one by one. Instead of doing so, we can combine the steps into a transformer object that we can then apply conveniently to multiple datasets. We have seen in the previous tutorial that some of the ways for us to combine transformations are `Pipeline` or `ColumnTransformers`.\n",
    "\n",
    "In this tutorial, we used `CountVectorizer` which is already a transformer class, but we also did custom preprocessing to generate character/word counts. We can write our custom transformer to do such custom preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLqM82g0HbhT"
   },
   "source": [
    "### Writing A Custom Transformer\n",
    "\n",
    "`sklearn.preprocessing` module provides [`FunctionTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html#sklearn.preprocessing.FunctionTransformer), which allows us to implement a transformer from an arbitrary function. However, if we do not have a specific function to implement as transformer, but want flexibility to implement our operations, we can write our transformer using two baseclasses from `sklearn`:\n",
    "\n",
    "1. [`BaseEstimator`](https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html): provides `get_params` and `set_params` functions. \n",
    "\n",
    "2. [`TransformerMixin`](https://scikit-learn.org/stable/modules/generated/sklearn.base.TransformerMixin.html): provides `fit_transform` function when we define our own `fit` and `transform` functions.\n",
    "\n",
    "In general, it is good to note that all estimators should specify all the parameters that can be set at the class level in their `__init__` as explicit keyword arguments. However, for our transformation, we are not storing any transformer parameters, and hence can also skip the `__init__` function.\n",
    "\n",
    "This [guide](https://scikit-learn.org/stable/developers/develop.html) is useful in understanding the specifications of each component in our custom transformer.\n",
    "\n",
    "Let's create a transformer that generates the `length`, `word_count` and `processed_word_count` features in the data. We name this class `FeatureCreator` and implement its `fit` and `transform` methods. Each of these methods takes in the estimator object `self`, sample data `X`, and labels `y` (for supervised learning).\n",
    "\n",
    "Complete the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C9bghWMlHbhT"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class FeatureCreator( BaseEstimator, TransformerMixin ):\n",
    "    \n",
    "    def fit( self, X, y=None ):\n",
    "        \n",
    "        # Nothing to be done for fit in this transformation\n",
    "        # Return fitted estimator\n",
    "        return self\n",
    "    \n",
    "    def transform( self, X, y=None ):\n",
    "        \n",
    "        # Initialize the DataFrame\n",
    "        self.df = pd.DataFrame()\n",
    "        self.df['length'] = X.apply( len )\n",
    "        \n",
    "        # TODO: Generate word_count and processed_word_count as you did previously\n",
    "               \n",
    "        # Return transformed data\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it on our original `sms_train` message data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kKhUkb8QHbhU",
    "outputId": "b3d043ea-ae1d-45b9-8dd6-f7a7060d747a"
   },
   "outputs": [],
   "source": [
    "featCreator = FeatureCreator()\n",
    "x_train = featCreator.fit_transform( sms_train['message'] )\n",
    "\n",
    "print( x_train.shape )\n",
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also write a simple transformer to convert a sparse matrix to array, which will come in useful later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YWJIqrZTHbhW"
   },
   "outputs": [],
   "source": [
    "class DenseTransformer( TransformerMixin ):\n",
    "\n",
    "    def fit( self, X, y=None, **fit_params ):\n",
    "        return self\n",
    "\n",
    "    def transform( self, X, y=None, **fit_params ):\n",
    "        return X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcAvzmQiHbhW"
   },
   "source": [
    "### FeatureUnion \n",
    "\n",
    "[`FeatureUnion`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html#sklearn.pipeline.FeatureUnion) is another class from `sklearn.pipeline`. It takes a list of transformer objects and creates a new transformer that combines their output. During fitting, each of these is fit to the data _independently_. The transformers are applied in parallel, and the feature matrices they output are concatenated side-by-side into a larger matrix.\n",
    "\n",
    "Do note here that the each transformer object is fit to _the entire data_. If we want to specify different transformers for different columns, we can go back to [`ColumnTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html#sklearn.compose.ColumnTransformer) covered in the previous tutorial.\n",
    "\n",
    "We will use `FeatureUnion` as well as `Pipeline` to combine our transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QgFV2xXHHbhX"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a `Pipeline` to chain together `CountVectorizer` (to generate vocabulary counts) and `DenseTransformer` (to convert the `CountVectorizer` sparse matrix output to array) as these should occur sequentially. Recall how to do this from previous tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QgFV2xXHHbhX"
   },
   "outputs": [],
   "source": [
    "# TODO: tf_pipeline = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine this pipeline with our earlier `FeatureCreator` using `FeatureUnion`, since they can be performed in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QgFV2xXHHbhX"
   },
   "outputs": [],
   "source": [
    "feats = FeatureUnion( [(\"lengths\", featCreator), (\"tf\", tf_pipeline)] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out on our original `sms_train` message data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "64F8fK7gHbhY",
    "outputId": "095fed59-e586-45f7-a33c-e2888ba3ee31"
   },
   "outputs": [],
   "source": [
    "feats.fit( sms_train['message'] )\n",
    "x_train = feats.fit_transform( sms_train.message )\n",
    "\n",
    "print( x_train.shape )\n",
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that it works, we can conveniently apply it to our `sms_test` message data too. __Remember that we should only perform `transform` without fitting on the test data.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OTzZ47o3EE6x"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chaining Preprocessing and Prediction\n",
    "\n",
    "We can also chain our complete preprocessing transformer with the classifier (e.g., the Complement Naive Bayes), which we can then apply to each new dataset to preprocess them and produce the prediction in one go. \n",
    "\n",
    "Which one should you use for this, `Pipeline` or `FeatureUnion`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sCGAXfdrHbha",
    "outputId": "59e8ee10-3b29-4723-9ec9-3e00afccf26b"
   },
   "outputs": [],
   "source": [
    "# TODO: text_clf = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we fit the text classifier with `sms_train` message data and the training labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sCGAXfdrHbha",
    "outputId": "59e8ee10-3b29-4723-9ec9-3e00afccf26b"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use it to run prediction on the `sms_test` message data, and evaluate the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xRGE68G1Hbhd",
    "outputId": "801da189-81b1-4d18-b3b4-58c64cb78ab4"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PT-8SYz8Ubpz"
   },
   "source": [
    "# Unsupervised Learning - Iris Classification Example\n",
    "\n",
    "## Dataset\n",
    "\n",
    "We make use of the iris dataset, which is available in scikit-learn. This dataset consists of 3 different types of irises (Setosa, Versicolour, and Virginica) with petal and sepal length data, stored in a 150 x 4 `numpy.ndarray`.\n",
    "\n",
    "We use the [dataset loading utilities](https://scikit-learn.org/stable/datasets.html) in `sklearn` to load the data, feature names and targets for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "72KfyP7WPSBU",
    "outputId": "9b93f8c6-c201-49e4-81e4-40f7911b56c2"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "iris_df = pd.DataFrame( data=iris.data, columns=iris.feature_names )\n",
    "target_df = pd.DataFrame( data=iris.target, columns=['species'] )\n",
    "iris_df = pd.concat( [iris_df, target_df], axis= 1 )\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual when we first examine a dataset, it is useful to check the descriptive statistics summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "Ix8bF0fCP_Wt",
    "outputId": "f9b5ebad-cee7-4b97-9fb2-17c7f1c9e241"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use visualization to explore this dataset. Here, we want to differentiate the species. \n",
    "\n",
    "Seaborn `pairplot` gives us a quick way to plot pairwise relationships in a dataset. The plot at position `(x,y)` where `x != y` gives the scatter plot of `y` against `x`, while the diagonal plots (`x == y`) show the marginal distribution of the data in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 741
    },
    "id": "cTJQuZC3QQed",
    "outputId": "d89acbdc-578b-4bea-8afb-53aeb1f40388"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "sns.pairplot( iris_df, hue='species' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PT-8SYz8Ubpz"
   },
   "source": [
    "## KMeans Clustering\n",
    "\n",
    "We try to identify how petal length and petal width vary with species. We construct the sample `X` (petal length and petal width) and labels `y` (species). For unsupervised learning, the labels are not used in training, but only in evaluating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jORAki_ihMjb"
   },
   "outputs": [],
   "source": [
    "X = iris_df[['petal width (cm)', 'petal length (cm)']].values\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jORAki_ihMjb"
   },
   "outputs": [],
   "source": [
    "y = iris_df['species'].values\n",
    "len( y )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxajy6Tz2Ged"
   },
   "source": [
    "We use [`sklearn.cluster.KMeans`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans) as our unsupervised learning method. \n",
    "\n",
    "First, instantiate the `KMeans` object.\n",
    "\n",
    "- As we know that there are 3 species to identify, we can specify `n_clusters=3`.\n",
    "\n",
    "- As the method involves random number generation for centroid initialization, as before, we should set `random_state` to a deterministic value to allow us to get reproducible results. We can reuse the `rng` variable we declared earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_4LsW5Zop_GM",
    "outputId": "adb28095-f158-4e40-ff01-46c344873f2a"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# TODO: km = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For unsupervised learning, we perform fitting only with the samples, without the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_4LsW5Zop_GM",
    "outputId": "adb28095-f158-4e40-ff01-46c344873f2a"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now access the clustering results via `labels_`, and add it as a column to our data for visualization purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DgYLq6Qsmvnc"
   },
   "outputs": [],
   "source": [
    "iris_labels = km.labels_\n",
    "iris_df['species_predict'] = iris_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the predicted labels in comparison with the true labels from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "1mzqyvJmp8ht",
    "outputId": "f72eb929-f8f0-46b0-be06-5000a3a1d36a"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f, axs = plt.subplots( 1, 2, figsize=(8, 4), gridspec_kw=dict(width_ratios=[4, 4]) )\n",
    "sns.scatterplot( data=iris_df, x=\"petal width (cm)\", y=\"petal length (cm)\", hue=\"species\", ax=axs[0]).set(title='Iris species labels (true)' )\n",
    "sns.scatterplot( data=iris_df, x=\"petal width (cm)\", y=\"petal length (cm)\", hue=\"species_predict\", ax=axs[1]).set(title='Iris species labels (predict)' )\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qe_ZNqvZroXq"
   },
   "source": [
    "### The Elbow Method\n",
    "\n",
    "What if we do not know that this dataset comprises 3 different types of irises? We can find the optimal number of types of irises using the elbow method.\n",
    "\n",
    "Our metric for how well the data is clustered for the KMeans model is *inertia*. Inertia is the sum of squared distances of samples to their closest cluster center, weighted by the sample weights if provided.\n",
    "\n",
    "A good model is one with low inertia and a low number of clusters (`k`). However, this is a tradeoff because as `K` increases, inertia decreases. The elbow refers to where the decrease in inertia begins to slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "id": "fkgceyVcq9rC",
    "outputId": "9943fa7b-6206-4fc1-f4c7-501d41768c04"
   },
   "outputs": [],
   "source": [
    "Sum_of_squared_distances = []\n",
    "K = range( 1, 15 )\n",
    "\n",
    "for k in K:\n",
    "    km = KMeans( n_clusters=k, n_init='auto' )\n",
    "    km = km.fit( X )\n",
    "    Sum_of_squared_distances.append( km.inertia_ )\n",
    "\n",
    "plt.plot( K, Sum_of_squared_distances, 'bx-' )\n",
    "plt.xlabel( 'k' )\n",
    "plt.ylabel( 'Sum_of_squared_distances' )\n",
    "plt.title( 'Elbow Method For Optimal k' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DZmhq7fPLQi"
   },
   "source": [
    "## Supervised Learning - Regression Example\n",
    "\n",
    "We will reuse the iris dataset to do one more supervised learning method, using regression. Instead of identifying the species as in the unsupervised learning example, we now want to predict how sepal length varies with sepal width, petal length, and petal width.\n",
    "\n",
    "The steps involved are:\n",
    "+ We split the dataset into training and test datasets.\n",
    "+ We fit the linear regression model on the training dataset.\n",
    "+ We use the linear regression model to predict on the test dataset.\n",
    "+ We calculate the mean squared error to understand goodness of fit.\n",
    "\n",
    "First, construct the sample `X` and labels `y` in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OZhrIzKcR0w0"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CB380o7YUhlq"
   },
   "source": [
    "It is good practice to scale the dataset using `StandardScaler()`, but since data preprocessing is not the focus of this tutorial, and the magnitude of all features is the same, we skip this step.\n",
    "\n",
    "Next, split the dataset using `train_test_split`, to use 33% of the data for testing. We can do the split for both samples and labels in one call. Revisit the [reference](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split) for an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OZhrIzKcR0w0"
   },
   "outputs": [],
   "source": [
    "# TODO: X_train, X_test, y_train, y_test = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lXBdWCWtgqlN"
   },
   "source": [
    "We perform a linear regression to fit the train dataset. We then find the $R^2$ and the coefficients of the regression equation. The attributes and methods for the `LinearRegression` class can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uIW6fy4ZSnQL",
    "outputId": "3df677a5-8e8e-4ed9-eee8-e4832d95223b"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit( X_train, y_train )\n",
    "lr.score( X_train, y_train ), lr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s7vDAnGIhEkt"
   },
   "source": [
    "We use the fitted linear regression model to perform predictions in the test dataset. We can also calculate metrics using the predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ew2RYFl4S1cM"
   },
   "outputs": [],
   "source": [
    "pred = lr.predict( X_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D-3KBMLhTNjV",
    "outputId": "78992c29-4c5c-40af-dbd0-06ffc1076bf4"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print( 'Mean Squared Error:', mean_squared_error( y_test, pred ))\n",
    "print( 'Mean Root Squared Error:', np.sqrt( mean_squared_error( y_test, pred )))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
