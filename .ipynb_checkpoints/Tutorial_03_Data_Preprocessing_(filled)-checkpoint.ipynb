{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZQkk_-8Uzmh"
   },
   "source": [
    "# Tutorial 3 (Week 4) - Data Preprocessing\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "After completing this tutorial, you should be able to:\n",
    "\n",
    "+ Perform data transformation using `sklearn.preprocessing`\n",
    "  + Perform standardization and normalization\n",
    "  + Encode ordinal and nominal values as numerical values\n",
    "  + Perform discretization\n",
    "  + Generate polynomial features\n",
    "+ Combine preprocessing steps for heterogenous data\n",
    "+ Handle missing values using `sklearn.impute`\n",
    "+ Perform dimensionality reduction using PCA\n",
    "\n",
    "References:\n",
    "- [scikit-tutorials](https://scikit-learn.org/stable/auto_examples/index.html#preprocessing)\n",
    "- [Preprocessing tutorial](https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing)\n",
    "- [Column transformers tutorial](https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html#sphx-glr-auto-examples-compose-plot-column-transformer-mixed-types-py)\n",
    "- [Pipelines tutorial](https://scikit-learn.org/stable/modules/compose.html#combining-estimators).\n",
    "\n",
    "\n",
    "We have learned data visualization in a previous tutorial. In practise, data cleaning and visualization go hand in hand, and are usually done together too. We will go over a few data cleaning strategies in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "scBIhwZEUzmq"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75jnkJDAUzmp"
   },
   "source": [
    "# Dataset\n",
    "\n",
    "Let us work on the heart disease dataset [\"Statlog (Heart)\"](https://archive.ics.uci.edu/dataset/145/statlog+heart). The csv file and txt files with the dataset info are available from this Tutorial folder.\n",
    "\n",
    "This dataset has 13 attributes and 1 label column (presence or absence of heart disease). In this tutorial, we are working on only data preprocessing, and not concerning ourselves with any model and its prediction. Thus for simplicity, we will work on the whole dataset without splitting it into training and testing datasets. \n",
    "\n",
    "Go ahead and read the dataset using Pandas as usual, loading it into a variable `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "S4NAmvE0Uzms",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'heart-statlog.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# TODO\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheart-statlog.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m data\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1213\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    788\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'heart-statlog.csv'"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "data = pd.read_csv( \"heart-statlog-.csv\" )\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KOt7a-CrUzmt"
   },
   "source": [
    "The `class` column indicates the presence and absence of disease, which we are not using for preprocessing in this tutorial. \n",
    "\n",
    "Use the `DataFrame.drop` function to drop the `class` column from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SGrpWVrTUzmu",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "data = data.drop( 'class', axis=1 )\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rename some of the columns for easier handling.\n",
    "\n",
    "```\n",
    "resting_blood_pressure               --> rest_BP\n",
    "serum_cholestoral                    --> cholesterol\n",
    "fasting_blood_sugar                  --> fast_sugar\n",
    "resting_electrocardiographic_results --> rest_ECG\n",
    "maximum_heart_rate_achieved          --> max_HR\n",
    "exercise_induced_angina              --> exer_angina\n",
    "number_of_major_vessels              --> vessels\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# TODO\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39mrename( columns\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresting_blood_pressure\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrest_BP\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mserum_cholestoral\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcholesterol\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfasting_blood_sugar\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfast_sugar\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresting_electrocardiographic_results\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrest_ECG\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximum_heart_rate_achieved\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_HR\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexercise_induced_angina\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexer_angina\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumber_of_major_vessels\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvessels\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     10\u001b[0m })\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "data = data.rename( columns={\n",
    "    'resting_blood_pressure' : 'rest_BP',\n",
    "    'serum_cholestoral' : 'cholesterol',\n",
    "    'fasting_blood_sugar' : 'fast_sugar',\n",
    "    'resting_electrocardiographic_results' : 'rest_ECG',\n",
    "    'maximum_heart_rate_achieved' : 'max_HR',\n",
    "    'exercise_induced_angina' : 'exer_angina',\n",
    "    'number_of_major_vessels' : 'vessels'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NrpiT-FNUzmu"
   },
   "source": [
    "The data has no missing values - this is stated in the accompanying txt file. Let's have a quick check on its descriptive statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QkOUypfaUzmv"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `heart-statlog.txt` file lists the data type of each attribute in the dataset: numerical (real), ordinal, binary, or categorical (nominal).\n",
    "\n",
    "We will perform different preprocessing operations:\n",
    "- Discretization on the `age` data\n",
    "- Normalization and Polynomial Feature Construction on the other numerical data\n",
    "- Encoding the ordinal and nominal data\n",
    "\n",
    "Let's save the column indices for each of these types in a list for our later use. Complete the code below using the information from `heart-statlog.txt`. _(Note that the numbering in the txt file starts from 1, while column indexing starts from 0 -- adjust accordingly.)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pwSiPnR6Uzmw"
   },
   "outputs": [],
   "source": [
    "# Discrete (the age column)\n",
    "disc_features = [0]\n",
    "\n",
    "# Numerical (the rest of the real columns)\n",
    "num_features = [3,4,7,9,11]\n",
    "\n",
    "# TODO\n",
    "\n",
    "# Ordinal\n",
    "# ordinal_features = ?\n",
    "ordinal_features = [10]\n",
    "\n",
    "# Binary\n",
    "# bin_features = ?\n",
    "bin_features = [1,5,8]\n",
    "\n",
    "# Categorical (nominal)\n",
    "# cat_features = ?\n",
    "cat_features = [6,2,12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i2TZ8f1zvVuQ"
   },
   "source": [
    "# Introduction to scikit-learn\n",
    "\n",
    "The [`scikit-learn`](https://scikit-learn.org/stable/index.html) library is a part of the SciPy (Scientific Python) group, which has a set of libraries created for scientific computing. The first part of the name refers to this origin of the library, while the second part refers to the discipline this library pertains to: Machine Learning. It is built on NumPy, and has extremely efficient and reusable codes. The library is included in the Anaconda distribution.\n",
    "\n",
    "## Transformers and Estimators\n",
    "\n",
    "__Transformers__ is a term used for classes in `scikit-learn` (or `sklearn`) that enable data transformations. `scikit-learn` provides a library of transformers, which may _clean_ (for preprocessing), _reduce_ (for unsupervised dimensionality reduction), _expand_ (for kernel approximation) or _generate_ (for feature extraction) feature representations.\n",
    "\n",
    "All standard transformers in `sklearn` have the following methods:\n",
    "\n",
    "- `fit`, which learns model parameters (e.g., mean and standard deviation for normalization) from a training set;\n",
    "- `transform`, which applies this transformation model to unseen data;\n",
    "- `fit_transform`, which models and transforms the training data simultaneously for convenience and efficiency.\n",
    "\n",
    "We will use transformers for scaling (standardization and normalisation) as well as for encoding in this tutorial.\n",
    "\n",
    "\n",
    "__Estimators__ is a term used for classes which manage the estimation and decoding of a model. Estimators must provide a `fit` method, and should provide `set_params` and `get_params`, although these are usually provided by inheritance from `base.BaseEstimator`. \n",
    "\n",
    "We will use an estimator for discretization in this tutorial. \n",
    "\n",
    "A useful estimator class, but which we are not using in this tutorial, is _Predictors_. It is an estimator supporting `predict` and/or `fit_predict`. This encompasses classifier, regressor, outlier detector and clusterer.\n",
    "\n",
    "\n",
    "## Preprocessing Module\n",
    "\n",
    "A package in `scikit-learn`, named [`sklearn.preprocessing`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing), provides several common utility functions and transformer classes to change raw feature vectors into a representation that is more suitable for tasks such as classification, regression, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pwSiPnR6Uzmw"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jmlCcfdzUzmx"
   },
   "source": [
    "# Standardization and Normalization\n",
    "\n",
    "_Standardization_ involves rescaling the features such that they have the properties of a standard normal distribution with a mean of zero and a standard deviation of one. Feature scaling through standardization (or Z-score normalization) can be an important preprocessing step for many machine learning algorithms. If a feature has a variance that is orders of magnitude larger than others, it might end up dominating the estimator, which might not learn well from other features. \n",
    "\n",
    "_Normalization_ is the process of scaling individual samples to have unit norm, independently of the distribution of the samples. \n",
    "\n",
    "Note that standardization is a _feature-wise_ operation, while normalization is a _sample-wise_ operation. \n",
    "\n",
    "## Standardization\n",
    "\n",
    "Let's perform standardization on the numerical columns. We can select these columns by passing the indices list we constructed earlier to `DataFrame.iloc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d-6xcSddUzmy",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.iloc[:,num_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `preprocessing` module provides the [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) utility class, which is a quick and easy way to perform standardization on an array-like dataset. The scaled data will have zero mean and a unit variance.\n",
    "\n",
    "The `fit_transform` method of `StandardScaler` works on each feature to first calculate the mean and variance of the feature (_fit_), then transforms the feature using the calculated mean and variance values as scaling parameters (_transform_). The method returns the transformed data as an array.\n",
    "\n",
    "Let's run this method on the numerical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wcyb7raaUzmx"
   },
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FtSmu3YKUzmy"
   },
   "outputs": [],
   "source": [
    "num_scaled = scaler.fit_transform( data.iloc[:,num_features] )\n",
    "num_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the standardization works: what are the mean values of the original columns, and what are the mean values of the transformed columns? Are the latter exactly zero?\n",
    "\n",
    "_(Note: For the multidimensional array, you will need to specify the axis in order to apply mean computation on individual columns.)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ssEvErt_Uzmy",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Original means\n",
    "data.iloc[:,num_features].mean( axis=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P4A5I4d2jtqr",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Transformed mean\n",
    "num_scaled.mean( axis=0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about the variance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Original variance\n",
    "data.iloc[:,num_features].var( axis=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Transformed variance\n",
    "num_scaled.var( axis=0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisation\n",
    "\n",
    "The `preprocessing` module has the [`Normalizer`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html#sklearn.preprocessing.Normalizer) utility class, which transforms individual samples to unit norm. We can specify which norm to use (i.e., how the unit norm is defined); the default is the `l2` norm (Euclidean).\n",
    "\n",
    "The `Normalizer` class also provides a `fit_transform` method. Run this method on our numerical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AXFEEsdrpvz6"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "preprocessing.Normalizer( norm='l2' ).fit_transform( data.iloc[:,num_features] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to perform normalisation is to use the [`normalize`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html#sklearn.preprocessing.normalize) method from the `preprocessing` module directly. Refer to the [Guide](https://scikit-learn.org/stable/modules/preprocessing.html#normalization) and try it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zfEsq0h4o5li"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "preprocessing.normalize( data.iloc[:,num_features], norm='l2' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L599gSPKUzmz"
   },
   "source": [
    "# Encoding Ordinal and Nominal Values\n",
    "\n",
    "Ordinal data can be encoded into numerical data using [`OrdinalEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html#sklearn.preprocessing.OrdinalEncoder). This results in a single column of integers (0 to `n_categories - 1`) per feature. \n",
    "\n",
    "For our ordinal data, we do not actually need to use ordinal encoder, as the data is already in integer form. We will just instantiate this class here for a later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KhBci-W0Uzmz"
   },
   "outputs": [],
   "source": [
    "ord_enc = preprocessing.OrdinalEncoder( categories='auto' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "walGvTSWUzmz"
   },
   "source": [
    "## One-Hot Encoding\n",
    "\n",
    "A common technique for encoding categorical variables is [`OneHotEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder). It transforms a categorical feature that has `n` possible values into `n` binary features. Exactly one of the binary features will have value 1 (corresponding to the feature value), and all others 0. \n",
    "\n",
    "For example, for a feature that has 4 categories named [1,2,3,4], the one-hot encoding will be:\n",
    "```\n",
    "1 -> [1, 0, 0, 0]\n",
    "\n",
    "2 -> [0, 1, 0, 0]\n",
    "\n",
    "3 -> [0, 0, 1, 0]\n",
    "\n",
    "4 -> [0, 0, 0, 1]\n",
    "```\n",
    "\n",
    "What will happen if the encoder encounters unknown categories during transform? When `handle_unknown='ignore'` is specified, no error will be raised but the resulting one-hot encoded columns for this feature will be all zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pQxwZz58Uzm0"
   },
   "outputs": [],
   "source": [
    "oh_enc = preprocessing.OneHotEncoder( categories='auto', handle_unknown='ignore' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply one-hot encoding on our categorical (nominal) data. Select the relevant columns from the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pQxwZz58Uzm0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "data.iloc[:, cat_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the `fit_transform` method of `OneHotEncoder` on those columns. How many columns do you expect to see in the output? (How many possible values does each feature have?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7lko2-jDUzm0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "oh_enc.fit_transform( data.iloc[:, cat_features] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert that sparse matrix output to NumPy multidimensional array so that we can view it, and save it as `data_cat` for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7lko2-jDUzm0"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "data_cat = oh_enc.fit_transform( data.iloc[:, cat_features] ).toarray()\n",
    "data_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlTa2nA4uKzB"
   },
   "source": [
    "As there are three transformed features, we will expect to see three 1 values in each row of the transformed data if there are no unknown categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "07wHz4dCUzm0"
   },
   "outputs": [],
   "source": [
    "data_cat[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlTa2nA4uKzB"
   },
   "source": [
    "We have earlier instantiated OneHotEncoder with `categories=auto`, so that it determines categories automatically from the data. We can check the `categories_` properties to see them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s2D3lKpNUzm0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "oh_enc.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vw23Bf56Uzm1"
   },
   "source": [
    "# Discretization\n",
    "\n",
    "Discretization (otherwise known as _quantization_ or _binning_) provides a way to partition continuous features into discrete values. One-hot encoded discretized features can make a model more expressive, while maintaining interpretability. \n",
    "\n",
    "In our example, we can perform discretization on the `age` column. Let's check its value range again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vw23Bf56Uzm1"
   },
   "outputs": [],
   "source": [
    "data['age'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vw23Bf56Uzm1"
   },
   "source": [
    "As the range of the column is 29 to 77, we can do a binning into 5 bins to express different age-groups. \n",
    "\n",
    "We will use the estimater class [`KBinsDiscretizer`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html#sklearn.preprocessing.KBinsDiscretizer) for this. Let's instantiate it with 5 bins and onehot encoding, specify a strategy that will give us equal-sized bins, and a subsample option that will use all the samples for computing the quantiles that determine the binning thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jg-ts7f-Uzm1"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# discretizer = ?\n",
    "discretizer = preprocessing.KBinsDiscretizer( n_bins=5, strategy='uniform', encode='onehot', subsample=None )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As `KBinDiscretizer` works with an array, we first need to convert the `age` column to a NumPy array of dimension `num_values` x 1, where the single column corresponds to the single feature. (You can use `reshape` to control the dimension.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# age_arr = ?\n",
    "age_arr = np.array(data['age']).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the discretizer's `fit` method to fit the data into bins. We can view the result by checking the `bin_edges_` property of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jg-ts7f-Uzm1"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "discretizer.fit( age_arr ).bin_edges_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the discretizer's `transform` method to discretize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aBPxj1MRKCHL"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "k = discretizer.transform( age_arr )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is a sparse matrix, which we can convert to NumPy multidimensional array for viewing. We will expect to see one-hot encoding format as we specified earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aBPxj1MRKCHL",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "k.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jvcECuwGUzm1"
   },
   "source": [
    "# Polynomial Feature Construction\n",
    "\n",
    "It is often useful to add complexity to the model by considering nonlinear features of the input data. The transformer class [`PolynomialFeatures`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn.preprocessing.PolynomialFeatures) allows us to generate higher order terms and interaction terms (representing joint effects of multiple features) to consider this non-linearity. \n",
    "\n",
    "Refer to the class documentation for the definitions and default values of the parameters. Let's instantiate this class with degree 2, exclude bias columns, and only produce interaction features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3kSK25OMUzm2"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# poly_tfr = ?\n",
    "poly_tfr = preprocessing.PolynomialFeatures( degree=2, include_bias=False, interaction_only=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply its `fit_transform` method to our numerical data columns. How many columns are there in the transformed data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3kSK25OMUzm2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# poly_feats = ?\n",
    "poly_feats = poly_tfr.fit_transform( data.iloc[:,num_features] )\n",
    "\n",
    "print( data.iloc[:,num_features].shape )\n",
    "print( poly_feats.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the names of the constructed features using the `get_feature_names_out` method of `PolynomialFeatures`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HGCLFrl1Uzm2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "poly_tfr.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those form the columns of the transformed data, comprising original and constructed features. Let's see the values on the first row before and after feature construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EDcdEzmUUzm2"
   },
   "outputs": [],
   "source": [
    "# TODO: Original feature values in first row\n",
    "data.iloc[0,num_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pP_i1HFhUzm2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Transformed feature values in first row\n",
    "poly_feats[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_dgDexPUzm3"
   },
   "source": [
    "# Putting It Together: Pipeline and ColumnTransformer\n",
    "\n",
    "Our dataset contains heterogeneous data types. As we have done various different preprocessing on different columns - how do we put it all together? A simple approach could be to stitch it all together in a new DataFrame. The following code snippet could do categorical encoding and binning. \n",
    "```\n",
    "new_data = pd.DataFrame()\n",
    "\n",
    "for i in range(6):\n",
    "    new_data['age_'+str(i)] = data_disc[:,i]\n",
    "new_data['sex'] = data.sex\n",
    "for i in range(4):\n",
    "    new_data['chest_pain_'+str(i)] = data_cat[:,i]\n",
    "new_data['restBP'] = data.restBP\n",
    "new_data['cholesterol'] = data.cholesterol\n",
    "new_data['fast_sugar'] = data.fast_sugar\n",
    "for i in range(3):\n",
    "    new_data['rest_ECG_'+str(i)] = data_cat[:,4+i]\n",
    "new_data['max_HR'] = data.max_HR\n",
    "new_data['exer_angina'] = data.exer_angina\n",
    "new_data['oldpeak'] = data.oldpeak\n",
    "new_data['slope'] = data.slope\n",
    "new_data['vessels'] = data.vessels\n",
    "for i in range(3):\n",
    "    new_data['thal_'+str(i)] = data_cat[:,7+i]\n",
    "    \n",
    "new_data.head()\n",
    "```\n",
    "\n",
    "However, as the number of preprocessing steps increase and change, this approach becomes difficult to scale. To rescue us from this difficulty, sklearn has the [`sklearn.pipeline`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.pipeline) and [`sklearn.compose`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.compose) packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lBzfgtICUzm3"
   },
   "outputs": [],
   "source": [
    "from sklearn import pipeline\n",
    "from sklearn import compose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "26CQgpH7Uzm3"
   },
   "source": [
    "[__`pipeline.Pipeline`__](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline) can be used to chain multiple _fixed_ steps into one. \n",
    "\n",
    "For example, our preprocessing steps for numeric columns are fixed: scaling, and doing polynomial feature creation. So we can essentially encapsulate these into a pipeline.\n",
    "\n",
    "Let's instantiate `Pipeline` to chain our `StandardScaler` and `PolynomialFeatures` transformers from earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MPbYcZF1Uzm3"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# numeric_transformer = ?\n",
    "numeric_transformer = pipeline.Pipeline( steps=[('scaler', scaler), ('poly', poly_tfr)] )\n",
    "\n",
    "'''\n",
    "This is the same as:\n",
    "\n",
    "numeric_transformer = pipeline.Pipeline( steps=[('scaler', preprocessing.StandardScaler()), \n",
    "                                                ('poly', preprocessing.PolynomialFeatures( degree=2, include_bias=False, interaction_only=True ))] )\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CG1TXlj42JA"
   },
   "source": [
    "We can blindly apply this numeric transformer (with `fit_transform`) to all our columns of data. Starting with 13 columns, how many columns will the fitted product have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qgvtnKff4SxY"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "numeric_transformer.fit_transform( data ).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CG1TXlj42JA"
   },
   "source": [
    "_(13*1 + 13C2 = 13+78 = 91)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ogPCWB7k5UfY"
   },
   "source": [
    "However, this is not very useful -- what is the meaning of a polynomial variable comprising a nominal variable multiplied by a real variable?\n",
    "\n",
    "[__`compose.ColumnTransformer`__](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html#sklearn.compose.ColumnTransformer) helps to perform different transformations for different columns of the data, within a Pipeline that is safe from data leakage and that can be parameterized. To each column, a different transformation can be applied, such as preprocessing for different types of data.\n",
    "\n",
    "Let's instantiate `ColumnTransformer` to combine the following transformers that we have seen earlier:\n",
    "- apply the numeric transformation `Pipeline` to our numeric features;\n",
    "- apply the discretizer to our discrete feature;\n",
    "- apply the one-hot encoder to our categorical features;\n",
    "- apply the ordinal encoder to our ordinal features.\n",
    "\n",
    "We can specify that we want all remaining columns that were not specified in transformers, but present in the data passed to fit, to be automatically passed through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mk1I09NqUzm4"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# preprocessor = ?\n",
    "preprocessor = compose.ColumnTransformer(\n",
    "                transformers=[\n",
    "                    ('num', numeric_transformer, num_features),\n",
    "                    ('disc', discretizer, disc_features),\n",
    "                    ('cat', oh_enc, cat_features),\n",
    "                    ('ord', ord_enc, ordinal_features)\n",
    "                ], remainder=\"passthrough\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run its `fit_transform` and check the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w1fT78O4Uzm4"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "preprocd_data = preprocessor.fit_transform(data)\n",
    "print( preprocd_data.shape )\n",
    "preprocd_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can convert the preprocessed data back to DataFrame format for our use in analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JPULFDvxzSyG"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "data_preprcd = pd.DataFrame(preprocd_data)\n",
    "data_preprcd.index = data.index\n",
    "data_preprcd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXAlgEAyzagu"
   },
   "source": [
    "# Dealing with Missing Values using `SimpleImputer`\n",
    "\n",
    "Since our dataset has no missing values, let us randomly remove some `age` values for the purpose of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8JN1x-5mzhBW"
   },
   "outputs": [],
   "source": [
    "data_drop = data.copy()\n",
    "data_drop.iloc[ np.random.randint(0, 268, size = 10).tolist(), 0 ] = np.nan\n",
    "data_drop.age.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5HKQ82L29fw"
   },
   "source": [
    "`sklearn.impute` package provides a [__`SimpleImputer`__](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html) that can help us fill these missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p-N9AAhY05ff"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instantiate the class to replace missing values with the mean of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p-N9AAhY05ff"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# imp_mean = ?\n",
    "imp_mean = SimpleImputer( missing_values=np.nan, strategy='mean' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run the `fit` method first and see the computed values, in particular the mean `age` which will be used to replace the values we removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p-N9AAhY05ff"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "imp_mean.fit( data_drop ).statistics_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the `transform` method to actually do the filling, and convert the result back to a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jn2-2F983ZKc"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# data_filled = ?\n",
    "data_filled = pd.DataFrame( imp_mean.transform( data_drop ))\n",
    "data_filled.columns, data_filled.index = data_drop.columns, data_drop.index\n",
    "data_filled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NumPy array format will enable us to see the entire column and check the replaced missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jn2-2F983ZKc"
   },
   "outputs": [],
   "source": [
    "data_filled.age.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQV9l3FTUzm4"
   },
   "source": [
    "# Image Data and PCA (Feature Decomposition)\n",
    "\n",
    "## Dataset\n",
    "\n",
    "Now let us work on image data, as we have already explored tabular, hierarchical and array data in the previous tutorials. Let us use the [Olivetti dataset](https://cam-orl.co.uk/facedatabase.html), which was used in the context of a face recognition project at AT&T Laboratories Cambridge. This dataset contains a set of face images of 40 different subjects. This dataset is available in `sklearn` itself. \n",
    "\n",
    "The below code will fetch the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BHE9WdVhUzm4"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "\n",
    "faces, targets = fetch_olivetti_faces( return_X_y=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned data `faces` is an array representation of the images, where each row corresponds to a ravelled face image of original size 64 x 64 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print( faces.shape )\n",
    "faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned `targets` are labels associated to each face image, ranging from 0-39 and correspond to the Subject IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3kLOh_ar33EJ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Matplotlib's function [`imshow`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html) to display data as an image. For example, let's display the 25th sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dZjgzPG1Uzm4"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image_shape = (64,64)\n",
    "plt.imshow( faces[24].reshape(image_shape), cmap=plt.cm.gray )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform standardization on this image data using `StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NdqZ6QiaUzm5"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "scaler = preprocessing.StandardScaler()\n",
    "faces_scaled = scaler.fit_transform( faces )\n",
    "print( faces_scaled.shape )\n",
    "faces_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformation will also be visible when we display the resulting data using Matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NdqZ6QiaUzm5"
   },
   "outputs": [],
   "source": [
    "# TODO: Display the scaled 25th sample\n",
    "plt.imshow( faces_scaled[24].reshape(image_shape), cmap=plt.cm.gray )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "or9hnEP2Uzm5"
   },
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "Principal Component Analysis is used to decompose a multivariate dataset in a set of successive orthogonal components that explain a maximum amount of the variance. It is a technique which essentially helps us to reduce the dimensionality of our dataset. \n",
    "\n",
    "The [`sklearn.decomposition`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.decomposition) module provides the transformer [`PCA`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA). It learns `n_components` in its `fit` method, and can be used on new data to project it on these components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l1c3gAZiUzm5"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# TODO: instantiate PCA and apply it to the standardized faces data\n",
    "# pca = ?\n",
    "pca = PCA()\n",
    "pca.fit( faces_scaled )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "or9hnEP2Uzm5"
   },
   "source": [
    "Let us find out how many components are sufficient to explain our faces dataset, by plotting the cumulative explained variance against the number of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l1c3gAZiUzm5"
   },
   "outputs": [],
   "source": [
    "plt.plot( np.cumsum( pca.explained_variance_ratio_ ))\n",
    "plt.xlabel( 'number of components' )\n",
    "plt.ylabel( 'cumulative explained variance' )\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sohXCoGiUzm5"
   },
   "source": [
    "We see that 100 components explain about 90% of the variance in the dataset. Thus, those 100 components might be sufficient for our downstream tasks like prediction. \n",
    "\n",
    "As we have image data however, we can actually view these orthogonal components that PCA has learnt. These are called _Eigenfaces_. A combination of these eigenfaces is usually sufficient to recreate the original sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bPaqtSEYUzm6",
    "outputId": "9b6b6046-8161-471c-ef7d-0a2c435554ec"
   },
   "outputs": [],
   "source": [
    "n_components=100\n",
    "h = w = 64\n",
    "\n",
    "print( \"Extracting the top %d eigenfaces from %d faces\" % (n_components, faces_scaled.shape[0]) )\n",
    "pca = PCA( n_components=n_components, svd_solver='randomized', whiten=True ).fit( faces_scaled )\n",
    "\n",
    "eigenfaces = pca.components_.reshape(( n_components, h, w ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RCqy4fwn9cox"
   },
   "outputs": [],
   "source": [
    "pca.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t_ykPcWLS7df"
   },
   "outputs": [],
   "source": [
    "eigenfaces.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how these Eigenfaces look like. The below code plots a gallery of portraits, with preset numbers of rows and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2hskJ_3EUzm6"
   },
   "outputs": [],
   "source": [
    "def plot_gallery( images, titles, h, w, n_row=3, n_col=5 ):\n",
    "    \"Helper function to plot a gallery of portraits\"\n",
    "    plt.figure( figsize=(1.8 * n_col, 2.4 * n_row) )\n",
    "    plt.subplots_adjust( bottom=0, left=.01, right=.99, top=.90, hspace=.35 )\n",
    "    for i in range( n_row * n_col ):\n",
    "        plt.subplot( n_row, n_col, i + 1 )\n",
    "        plt.imshow( images[i].reshape((h, w)), cmap=plt.cm.gray )\n",
    "        plt.title( titles[i], size=12 )\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "\n",
    "eigenface_titles = [\"eigenface %d\" % i for i in range(eigenfaces.shape[0])]\n",
    "plot_gallery( eigenfaces, eigenface_titles, h, w )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bCVTMmjQvt4f"
   },
   "source": [
    "Eigenfaces are eigenvectors used in the computer vision problem of human face recognition. They are the principal components of a distribution of faces. They determine the variance in faces in a dataset, and the variances can be used to encode and decode a face in machine learning. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
