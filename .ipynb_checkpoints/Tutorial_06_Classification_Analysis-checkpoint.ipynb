{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51866f53",
   "metadata": {},
   "source": [
    "# Tutorial 6 (Week 8) - Classification Analysis\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "After completing this tutorial, you should be able to:\n",
    "\n",
    "+ Understand model thresholding\n",
    "+ Use sklearn to plot ROC curves in binary classification\n",
    "+ Use sklearn to calculate AUROC\n",
    "+ Use sklearn to plot ROC curves in multi-class classification\n",
    "\n",
    "This tutorial is based on this [ROC and AUC tutorial](https://www.kaggle.com/code/jacoporepossi/tutorial-roc-auc-clearly-explained) and the Scikit-learn [ROC User Guide](https://scikit-learn.org/stable/modules/model_evaluation.html#receiver-operating-characteristic-roc)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0a35d5",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [Dataset](#Dataset)\n",
    "* [Confusion Matrix](#Confusion-Matrix)\n",
    "* [Model Thresholds](#Model-Thresholds)\n",
    "* [Receiver Operating Characteristic (ROC)](#ROC)\n",
    "* [Area Under the Curve (AUC or AUROC)](#AUROC)\n",
    "* [ROC Curve Application -- Comparability](#Comparability)\n",
    "* [AUROC Properties](#AUROC-Properties)\n",
    "* [AUROC in Multi-Class Classification](#Multi-Class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afec259",
   "metadata": {},
   "source": [
    "## Dataset <a class=\"anchor\" id=\"Dataset\"></a>\n",
    "\n",
    "Let us first create a toy dataset for experimenting. The Scikit-learn `datasets` module has a handy function [`make_classification()`](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) to generate a random n-class classification problem. \n",
    "\n",
    "We create a dataset with 50 samples (with default number of features) and 2 classes, with 40:60 proportion of samples assigned to each class. We set the class separation to be 0.1, which is a factor determining how spread out the classes are (the larger the value, the more spread out and the easier the classification task is). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d442fd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "x, y = make_classification( n_classes = 2,\n",
    "                            class_sep = 0.1,\n",
    "                            n_samples = 50,\n",
    "                            weights = [0.4, 0.6],\n",
    "                            random_state = 42 )\n",
    "\n",
    "# x contains the samples\n",
    "print( x.shape )\n",
    "\n",
    "# y contains the labels\n",
    "print( y.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55ca1d0",
   "metadata": {},
   "source": [
    "Let's visualize the generated data, taking 2 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458bd2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Associate label value 0 (resp. 1) with the color blue (resp. orange) and label text 'positive' (resp. 'negative')\n",
    "for c, i, t in zip( ['blue', 'orange'], [0, 1], ['positive', 'negative'] ):\n",
    "    plt.scatter( x[y==i, 0], x[y==i, 1], color=c, alpha=.5, label=t )\n",
    "\n",
    "plt.legend()\n",
    "plt.title( 'Data' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030d5919",
   "metadata": {},
   "source": [
    "Next, we make a random prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1913ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "y_pred = np.random.choice([0, 1], size=(50))\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4047e025",
   "metadata": {},
   "source": [
    "## Confusion Matrix <a class=\"anchor\" id=\"Confusion-Matrix\"></a>\n",
    "\n",
    "Let's build the confusion matrix for our random prediction. We can use `sklearn.metrics.confusion_matrix` to get the raw counts as we have seen in Tutorial 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00a551e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# c = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9aba52",
   "metadata": {},
   "source": [
    "We can then calculate the True Positive Rate (TPR) and False Positive Rate (FPR) from those counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b01150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# tn, fp, fn, tp = ?\n",
    "# tpr = ?\n",
    "# fpr = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d0f287",
   "metadata": {},
   "source": [
    "## Model Thresholds <a class=\"anchor\" id=\"Model-Thresholds\"></a>\n",
    "\n",
    "The goal of classification is to predict a class label. However, many machine learning algorithms predict a probability or scoring of class membership, and we need to interpret this to map the prediction to a specific class label. This mapping is achieved using a _threshold_ (e.g., 0.5), where all predictions at or above the threshold are mapped to one class and all other values are mapped to another class.\n",
    "\n",
    "In scikit-learn we can generally use two functions to perform prediction on new data: `predict` and `predict_proba`.\n",
    "\n",
    "The `predict_proba` function returns a two-dimensional array (`n_samples` x `n_classes`), containing the estimated probabilities for each instance and each class. For example, a prediction for 4 samples with 2 possible classes (0 or 'positive', and 1 or 'negative') may look like this:\n",
    "\n",
    "```\n",
    "array( [[0.90, 0.10],\n",
    "        [0.25, 0.75],\n",
    "        [0.78, 0.22],\n",
    "        [0.05, 0.95]])\n",
    "```\n",
    "This prediction says that the first sample has 90% probability of belonging to the positive class (and 10% probability of belonging to the negative class), the second sample has 75% probability of belonging to the negative class (and 25% probability of belonging to the positive class), and so on.\n",
    "\n",
    "The `predict` function simply gives the class with the maximum probability. For the above example, it will return:\n",
    "\n",
    "```\n",
    "array( [0, 1, 0, 1] )\n",
    "```\n",
    "\n",
    "Using `predict_proba`, we can adjust how our model predicts a class or the other by varying the threshold. For instance, we can set threshold = 0.8 for the negative class, so that the model will predict the negative class only for samples that have probability >= 80% of belonging to the negative class. \n",
    "\n",
    "For the above example, the model will predict the second sample as the positive class instead of the negative class as previously, since it has only 75% (<80%) probability of belonging to the negative class.\n",
    "\n",
    "Varying the thresholds will thus _affect TPR and FPR_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716282dc",
   "metadata": {},
   "source": [
    "## Receiver Operating Characteristic (ROC) <a class=\"anchor\" id=\"ROC\"></a>\n",
    "\n",
    "ROC curves are typically used in _binary classification_ to study the output of a classifier. An ROC curve is built by plotting FPR on the X axis and TPR on the Y axis using different threshold values.\n",
    "\n",
    "Let's test it with a simple model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce35583d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit( x, y )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f718139",
   "metadata": {},
   "source": [
    "We call `predict_proba()` and take the first column of the result, i.e., the probabilities of the samples belonging to the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a03f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = model.predict_proba(x)[:, 1]\n",
    "y_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23da46b8",
   "metadata": {},
   "source": [
    "Now let's build the ROC curve, using the sklearn function [`roc_curve()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) from the `metrics` module. We provide it with the true label and the predicted `y_score`, and it essentially:\n",
    "- determines various threshold values, and\n",
    "- calculates the FPR and TPR values for each threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc50760",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve( y, y_score )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983f080c",
   "metadata": {},
   "source": [
    "We can gather them in a DataFrame for ease of viewing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9ad517",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_df = pd.DataFrame( zip(fpr, tpr, thresholds), columns = [\"FPR\", \"TPR\", \"Threshold\"])\n",
    "\n",
    "print( roc_df.shape )\n",
    "roc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a65e605",
   "metadata": {},
   "source": [
    "As explained in the API reference, `thresholds[0]` represents no instances being predicted and is arbitrarily set to `np.inf`.\n",
    "\n",
    "_Question to ponder: How do you think the `roc_curve()` function determines what threshold values to use?_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a0fd91",
   "metadata": {},
   "source": [
    "Let's now create the plot with FPR on the X axis and TPR on the Y axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f6f2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot( fpr, tpr )\n",
    "\n",
    "ax.set_xlabel( 'False positive rate' )\n",
    "ax.set_ylabel( 'True positive rate' )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37eda367",
   "metadata": {},
   "source": [
    "The top left corner of the plot is the \"ideal\" point: FPR of zero and TPR of one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8804c3",
   "metadata": {},
   "source": [
    "## Area Under the Curve (AUC or AUROC) <a class=\"anchor\" id=\"AUROC\"></a>\n",
    "\n",
    "The area underneath the entire ROC curve is called AUROC (or AUC) and is always represented as a value between 0 to 1.\n",
    "\n",
    "We can see that the nearer the ROC curve is to the \"ideal\" point, the larger the area under the curve will be. Thus, we usually want to maximize AUROC, as this means achieving highest possible TPR and lowest possible FPR. \n",
    "\n",
    "_Question to ponder: Is it always the case?_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efece3e",
   "metadata": {},
   "source": [
    "We can compute the AUROC using the sklearn function [`roc_auc_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html), giving it the prediction scores, similar to how we use `roc_curve()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814066f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "auroc = roc_auc_score( y, y_score )\n",
    "auroc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2d4e16",
   "metadata": {},
   "source": [
    "We can redraw the ROC curve plot with the area filled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca8090a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(fpr, tpr)\n",
    "\n",
    "ax.fill_between( fpr, tpr, step=\"pre\", alpha=0.4 )\n",
    "\n",
    "ax.set_xlabel( 'False positive rate' )\n",
    "ax.set_ylabel( 'True positive rate' )\n",
    "\n",
    "# Project points to x-axis\n",
    "plt.vlines( fpr, 0, tpr, linestyle=\"dashed\" )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0c381f",
   "metadata": {},
   "source": [
    "The dashed lines illustrate how the AUROC calculation is done -- the function basically calculates the area of each rectangle and sums them up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecc951e",
   "metadata": {},
   "source": [
    "## ROC Curve Application - Comparability <a class=\"anchor\" id=\"Comparability\"></a>\n",
    "\n",
    "The ROC curve is valuable mainly for two reasons:\n",
    "\n",
    "- It lets us select an optimal threshold for that model, and \n",
    "- It gives us a visual way to compare different classifiers.\n",
    "\n",
    "Let's illustrate the second point by using another classifier together with the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346a5389",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier( max_depth=1 )\n",
    "rf.fit( x, y )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d946df",
   "metadata": {},
   "source": [
    "Following the same steps as the first classifier, perform the prediction, build the ROC curve, and compute the AUROC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ccfa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# y_score_rf = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dd9b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# fpr_rf, tpr_rf, thresholds_rf = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2257ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# auroc_rf = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c76f2e",
   "metadata": {},
   "source": [
    "Now we can put them together, and also add the ROC curve of a hypothetical _perfect classifier_, i.e., one that will always have TPR = 1 regardless of the FPR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589a8a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Logistic Regression\n",
    "ax.plot( fpr, tpr )\n",
    "\n",
    "# Random Forest\n",
    "ax.plot( fpr_rf, tpr_rf )\n",
    "\n",
    "# Perfect Classifier\n",
    "ax.plot([0, 0, 1], [0, 1, 1])\n",
    "\n",
    "ax.set_xlabel( 'False positive rate' )\n",
    "ax.set_ylabel( 'True positive rate' )\n",
    "ax.legend(\n",
    "    [\n",
    "        'LogisticRegression - AUROC {:.3f}'.format(auroc),\n",
    "        'RandomForest - AUROC {:.3f}'.format(auroc_rf),\n",
    "        'Perfect classifier'\n",
    "    ]\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa8ecdf",
   "metadata": {},
   "source": [
    "We can see that, compared to the LogisticRegression classifier, the RandomForest classifier has:\n",
    "- higher ROC curve (closer to the perfect classifier), and\n",
    "- larger AUROC value.\n",
    "\n",
    "Therefore, we can say that the RandomForest classifier is doing a better job than the LogisticRegression classifier at classifying the positive class in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a9c71e",
   "metadata": {},
   "source": [
    "## AUROC Properties <a class=\"anchor\" id=\"AUROC-Properties\"></a>\n",
    "\n",
    "AUROC has the following properties that are desirable for measuring classification performance:\n",
    "\n",
    "- Scale-invariant: It measures how well predictions are ranked, rather than their absolute values.\n",
    "- Threshold-invariant: It measures the quality of the model's predictions irrespective of what classification threshold is chosen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4c32b7",
   "metadata": {},
   "source": [
    "## AUROC in Multi-Class Classification <a class=\"anchor\" id=\"Multi-Class\"></a>\n",
    "\n",
    "We have so far talked about binary classification. The `roc_auc_score` function can also be used in _multi-class classification_. Scikit-learn currently supports two averaging strategies:\n",
    "\n",
    "- The __one-vs-one (OvO)__ algorithm computes the average of the pairwise AUROC scores.\n",
    "\n",
    "- The __one-vs-rest (OvR)__ algorithm computes the average of the AUROC scores for each class against all other classes.\n",
    "\n",
    "In both cases, the predicted labels are provided in an array with values from 0 to` n_classes`, and the scores correspond to the probability estimates that a sample belongs to a particular class. Both algorithms support weighting uniformly (`average='macro'`) and by prevalence (`average='weighted'`).\n",
    "\n",
    "Let's illustrate using the iris dataset which we have seen in past tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64e6ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, datasets\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6d9c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Binarize the output\n",
    "y = label_binarize(y, classes=[0, 1, 2])\n",
    "n_classes = y.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1c6979",
   "metadata": {},
   "source": [
    "Let's shuffle and split it into training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81305ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.5, random_state=0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a568de39",
   "metadata": {},
   "source": [
    "For the purpose of this illustration, we use [`OneVsRestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html) with [`SVC`](https://scikit-learn.org/1.0/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) as the estimator to learn from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0874c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = np.random.RandomState(0)\n",
    "\n",
    "classifier = OneVsRestClassifier(\n",
    "    svm.SVC( kernel=\"linear\", probability=True, random_state=random_state )\n",
    ")\n",
    "y_score = classifier.fit( X_train, y_train ).decision_function( X_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eb402f",
   "metadata": {},
   "source": [
    "We can now perform the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbf6662",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = classifier.predict_proba( X_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b21c09",
   "metadata": {},
   "source": [
    "Let's calculate the AUROC values for each of the algorithms, OvO and OvR, each with macro-averaging and weighted-averaging. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497c733a",
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_roc_auc_ovo = roc_auc_score( y_test, y_prob, multi_class=\"ovo\", average=\"macro\" )\n",
    "weighted_roc_auc_ovo = roc_auc_score( y_test, y_prob, multi_class=\"ovo\", average=\"weighted\" )\n",
    "print(\n",
    "    \"One-vs-One ROC AUC scores:\\n{:.6f} (macro),\\n{:.6f} \"\n",
    "    \"(weighted by prevalence)\".format(macro_roc_auc_ovo, weighted_roc_auc_ovo)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed62265e",
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_roc_auc_ovr = roc_auc_score( y_test, y_prob, multi_class=\"ovr\", average=\"macro\" )\n",
    "weighted_roc_auc_ovr = roc_auc_score( y_test, y_prob, multi_class=\"ovr\", average=\"weighted\" )\n",
    "print(\n",
    "    \"One-vs-Rest ROC AUC scores:\\n{:.6f} (macro),\\n{:.6f} \"\n",
    "    \"(weighted by prevalence)\".format(macro_roc_auc_ovr, weighted_roc_auc_ovr)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
